{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Besh Vignesh\\anaconda3\\Lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "rcParams['figure.figsize']=14,6\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('picklefile/prepare_data_set.pickle', 'rb') as data:\n",
    "    df = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment']=df['sentiment'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This kindle is light and easy to use especiall...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Didnt know how much i'd use a kindle so went f...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am 100 happy with my purchase. I caught it o...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Solid entry level Kindle. Great for kids. Gift...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This make an excellent ebook reader. Don't exp...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  rating  sentiment  \\\n",
       "0  This kindle is light and easy to use especiall...     5.0          1   \n",
       "1  Didnt know how much i'd use a kindle so went f...     4.0          1   \n",
       "2  I am 100 happy with my purchase. I caught it o...     5.0          1   \n",
       "3  Solid entry level Kindle. Great for kids. Gift...     5.0          1   \n",
       "4  This make an excellent ebook reader. Don't exp...     5.0          1   \n",
       "\n",
       "   text length  \n",
       "0           63  \n",
       "1          107  \n",
       "2          757  \n",
       "3          176  \n",
       "4          158  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I was looking for a kindle whitepaper. I saw o...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Looking at the picture and seeing it was 8th g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>it would not load my books proper. took a doze...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>The screen is too dark, and cannot adjust the ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>I have to say it was a little confusing and fr...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>This is exactly like any other usb power charg...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>Amazon should include this charger with the Ki...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>Love my Kindle Fire but I am really disappoint...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>I was surprised to find it did not come with a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>to spite the fact that i have nothing but good...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2510 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  rating  sentiment  \\\n",
       "18    I was looking for a kindle whitepaper. I saw o...     1.0          0   \n",
       "67    Looking at the picture and seeing it was 8th g...     1.0          0   \n",
       "75    it would not load my books proper. took a doze...     2.0          0   \n",
       "86    The screen is too dark, and cannot adjust the ...     2.0          0   \n",
       "136   I have to say it was a little confusing and fr...     2.0          0   \n",
       "...                                                 ...     ...        ...   \n",
       "7191  This is exactly like any other usb power charg...     1.0          0   \n",
       "7192  Amazon should include this charger with the Ki...     1.0          0   \n",
       "7193  Love my Kindle Fire but I am really disappoint...     1.0          0   \n",
       "7194  I was surprised to find it did not come with a...     1.0          0   \n",
       "7195  to spite the fact that i have nothing but good...     1.0          0   \n",
       "\n",
       "      text length  \n",
       "18            511  \n",
       "67            651  \n",
       "75             96  \n",
       "86             56  \n",
       "136           420  \n",
       "...           ...  \n",
       "7191          155  \n",
       "7192          255  \n",
       "7193          314  \n",
       "7194          231  \n",
       "7195          468  \n",
       "\n",
       "[2510 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['sentiment']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9372, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to spite the fact that i have nothing but good things to say about amazon and anthing i've ever gotten from them. and that i love my fire. i find it greedy that the wall charger doesn't come with the kindle. not everyone, ok most people, but still not everyone has a usb port to plug into. i'm taking my charger back. i think amazon should make things right and let anyone who purchased a kindle without a charger have one for free, or credit those who had to buy one.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[7195]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 7029 training examples and 2343 validation examples. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'],df['sentiment'], test_size=0.25, random_state=101)\n",
    "\n",
    "print('Loading %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Creating a function to do text preprocessing\n",
    "## 2. Remove non-character such as digits and symbols\n",
    "## 3. Convert to lower case\n",
    "## 4. Remove stop words such as \"the\" and \"and\" if needed\n",
    "## 5. Convert to root words by stemming if needed\n",
    "\n",
    "def cleanText(text, remove_stopwords=False, Lemmatization=False,split_text=False):\n",
    "    \n",
    "    # Removing non-character\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # converting to lower case\n",
    "    words = letters_only.lower().split()  \n",
    "    \n",
    "    \n",
    "    #Lemmatization\n",
    "    if Lemmatization==True: \n",
    "        Lemmatizer = WordNetLemmatizer('english') \n",
    "        words = [Lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "    # Removing stopword\n",
    "    if remove_stopwords==True: \n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Stemming\n",
    "    #if stemming==True: \n",
    "     #   stemmer = PorterStemmer('english') \n",
    "     #   words = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    \n",
    "    # Spliting text    \n",
    "    if split_text==True:  \n",
    "        return (words)\n",
    "    \n",
    "    return( \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing a cleaned review in the training set : \n",
      "\n",
      " it s not worth to buy i got issue with in one month from purchase date one ear sound stopped suddenly please don t waste your money on this product\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing text data in training set and validation set\n",
    "\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Showing a cleaned review in the training set : \\n\\n',  X_train_cleaned[344])\n",
    "    \n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 7681 \n",
      "\n",
      "Show some feature names : \n",
      " ['aa', 'cansilation', 'dollars', 'grow', 'lowes', 'population', 'sheets', 'trusted']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting and transform the training data to a document-term matrix using CountVectorizer\n",
    "\n",
    "countVect = CountVectorizer() \n",
    "X_train_countVect = countVect.fit_transform(X_train_cleaned)\n",
    "print(\"Number of features : %d \\n\" %len(countVect.get_feature_names())) #6378 \n",
    "print(\"Show some feature names : \\n\", countVect.get_feature_names()[::1000])\n",
    "\n",
    "\n",
    "# Train MultinomialNB classifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_countVect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_countVect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline_lr = Pipeline([('lr_classifier',LogisticRegression(random_state=0))])\n",
    "\n",
    "pipeline_dt = Pipeline([('dt_classifier',DecisionTreeClassifier())])\n",
    "\n",
    "pipeline_rf = Pipeline([('rf_classifier',RandomForestClassifier())])\n",
    "\n",
    "pipeline_mnb = Pipeline([('mnb_classifier',MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [pipeline_lr,pipeline_dt,pipeline_rf,pipeline_mnb]\n",
    "best_accuracy=0.0\n",
    "best_classifier=0\n",
    "best_pipeline=\"\"\n",
    "pipe_dict = {0:'Logistic Regression',1:'Decision Tree',2:'RandomForest',3:'MultiNomialNB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train_countVect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Accuracy: 0.939820742637644\n",
      "Decision Tree Test Accuracy: 0.830559112249253\n",
      "RandomForest Test Accuracy: 0.9086641058472045\n",
      "MultiNomialNB Test Accuracy: 0.9321382842509603\n"
     ]
    }
   ],
   "source": [
    "for i,model in enumerate(pipelines):\n",
    "    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(countVect.transform(X_test_cleaned),y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with best accuracy:Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "for i,model in enumerate(pipelines):\n",
    "    if model.score(countVect.transform(X_test_cleaned),y_test)>best_accuracy:\n",
    "        best_accuracy = model.score(countVect.transform(X_test_cleaned),y_test)\n",
    "        best_classifier = i\n",
    "        best_pipeline = model\n",
    "print('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train_countVect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEvaluation(predictions):\n",
    "    \n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.9398\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       645\n",
      "           1       0.95      0.96      0.95      1163\n",
      "           2       0.97      0.95      0.96       535\n",
      "\n",
      "    accuracy                           0.94      2343\n",
      "   macro avg       0.94      0.93      0.94      2343\n",
      "weighted avg       0.94      0.94      0.94      2343\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 579   54   12]\n",
      " [  43 1116    4]\n",
      " [  19    9  507]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on validaton set\n",
    "\n",
    "predictions = classifier.predict(countVect.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 0, 1, 2, 0, 1, 0,\n",
       "       1, 2, 1, 2, 1, 0, 2, 1, 2, 2, 0, 2, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Not Recommended Delivered defective product. IFB customer care executive are very unprofessional. Even after several requests they have not sent any technician to solve the issue.Very disappointed. Not a trust worthy brand.Please don't go for it.\"\n",
    "clean_text = cleanText(text)\n",
    "clean_text1 = clean_text.split()\n",
    "clean_text1\n",
    "pred = mnb.predict(countVect.transform(clean_text1))\n",
    "\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 16, 17], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "End_prediction = np.bincount(pred)\n",
    "End_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 17]\n",
      "Positive words: 16\n",
      "Negative words: 23\n",
      "it may be a negative review or compliant or both\n"
     ]
    }
   ],
   "source": [
    "End_prediction = np.bincount(pred)\n",
    "End_prediction_dict = {0:End_prediction[0],1:End_prediction[1],2:End_prediction[2]}\n",
    "if End_prediction_dict[0]+End_prediction_dict[2] > End_prediction_dict[1]: \n",
    "    print(np.delete(End_prediction,1))\n",
    "    print(\"Positive words:\",End_prediction_dict[1])\n",
    "    print(\"Negative words:\",End_prediction_dict[0]+End_prediction_dict[2])\n",
    "    print(\"it may be a negative review or compliant or both\")\n",
    "else:\n",
    "    print(\"Positive words:\",End_prediction_dict[1])\n",
    "    print(\"Negative words:\",End_prediction_dict[0]+End_prediction_dict[2])\n",
    "    print(\"It is a positive review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 2459 \n",
      "\n",
      "Show some feature names : \n",
      " ['00', 'head', 'son']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Fit and transform the training data to a document-term matrix using TfidfVectorizer \n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=5) # Taking a minimum document frequency of 5\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names()))\n",
    "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\n",
    "\n",
    "#  Instatiating and fitting Logistic Regression object\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features with smallest coefficients :\n",
      "['sound' 'love' 'product' 'great' 'easy' 'earphones' 'is' 'ear' 'working'\n",
      " 'earphone']\n",
      "\n",
      "Top 10 features with largest coefficients : \n",
      "['batteries' 'slow' 'returned' 'not' 'these' 'last' 'amazon' 'dead'\n",
      " 'terrible' 'remote']\n"
     ]
    }
   ],
   "source": [
    "# Looking at the top 10 features with smallest and the largest coefficients\n",
    "\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "sorted_coef_index = lr.coef_[0].argsort()\n",
    "print('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best paramenter set is : \n",
      " {'lr__C': 10, 'tfidf__max_features': None, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None}\n",
      "\n",
      "Accuracy on validation set: 0.9518\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       645\n",
      "           1       0.94      0.98      0.96      1163\n",
      "           2       0.99      0.96      0.97       535\n",
      "\n",
      "    accuracy                           0.95      2343\n",
      "   macro avg       0.96      0.94      0.95      2343\n",
      "weighted avg       0.95      0.95      0.95      2343\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 578   64    3]\n",
      " [  18 1141    4]\n",
      " [  11   13  511]]\n"
     ]
    }
   ],
   "source": [
    "estimators = [(\"tfidf\", TfidfVectorizer()), (\"lr\", LogisticRegression())]\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "\n",
    "# Grid search\n",
    "params = {\"lr__C\":[0.1, 1, 10], #regularization param of logistic regression\n",
    "          \"tfidf__min_df\": [1, 3], #min count of words \n",
    "          \"tfidf__max_features\": [1000, None], #max features\n",
    "          \"tfidf__ngram_range\": [(1,1), (1,2)], #1-grams or 2-grams\n",
    "          \"tfidf__stop_words\": [None, \"english\"]} #use stopwords or don't\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid.fit(X_train_cleaned, y_train)\n",
    "print(\"The best paramenter set is : \\n\", grid.best_params_)\n",
    "\n",
    "\n",
    "# Evaluate on the validaton set\n",
    "predictions = grid.predict(X_test_cleaned)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 0, 1, 2, 2, 2, 1,\n",
       "       0, 1, 1, 2, 2, 0, 2, 1, 2, 2, 0, 2, 0, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Not Recommended Delivered defective product. IFB customer care executive are very unprofessional. Even after several requests they have not sent any technician to solve the issue.Very disappointed. Not a trust worthy brand.Please don't go for it.\"\n",
    "clean_text = cleanText(text)\n",
    "clean_text1 = clean_text.split()\n",
    "clean_text1\n",
    "pred1 = grid.predict(clean_text1)\n",
    "\n",
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 15, 19], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "End_prediction = np.bincount(pred1)\n",
    "End_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 19]\n",
      "Positive words: 15\n",
      "Negative words: 24\n",
      "it may be a negative review or compliant or both\n"
     ]
    }
   ],
   "source": [
    "End_prediction = np.bincount(pred1)\n",
    "End_prediction_dict = {0:End_prediction[0],1:End_prediction[1],2:End_prediction[2]}\n",
    "if End_prediction_dict[0]+End_prediction_dict[2]+4 > End_prediction_dict[1]: \n",
    "    print(np.delete(End_prediction,1))\n",
    "    print(\"Positive words:\",End_prediction_dict[1])\n",
    "    print(\"Negative words:\",End_prediction_dict[0]+End_prediction_dict[2])\n",
    "    print(\"it may be a negative review or compliant or both\")\n",
    "else:\n",
    "    print(\"Positive words:\",End_prediction_dict[1])\n",
    "    print(\"Negative words:\",End_prediction_dict[0]+End_prediction_dict[2])\n",
    "    print(\"It is a positive review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the fire tablet is great we just bought our se...\n",
       "1    unexpected sound quality too much treble uncom...\n",
       "2    i was so excited to start streaming k with thi...\n",
       "3    many features packed into a low price good as ...\n",
       "4    i picked up the show on a black friday deal i ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_cleaned_text= pd.Series(X_train_cleaned+X_test_cleaned)\n",
    "df_cleaned_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted= df[['rating', 'sentiment']]\n",
    "df_predicted['cleaned Review']= df_cleaned_text\n",
    "df_predicted= df_predicted[['cleaned Review', 'rating','sentiment']]\n",
    "df_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
